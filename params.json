{"note":"Don't delete this file! It's used internally to help with page regeneration.","name":"Mechanic","body":"Overview\r\n--------\r\n\r\nThe Mechanic is a task management system and host software framework developed to help \r\nin conducting massive numerical simulations. It provides powerful and flexible user API \r\nwith unified data storage and management. It relies on the core-module approach, which \r\nallows to separate numerical problem from the common programming tasks, such as setup, \r\nstorage, task management, splitting the workload, checkpointing etc. From this point of \r\nview it allows the user to focus on the numerical part of the scientific problem only, \r\nwithout digging into MPI or advanced data storage. Since the user API is written in C, \r\nit allows to easily adapt any code developed with a C-interoperable programming language,\r\nsuch as C++, Fortran2003+, OpenCL, CUDA etc.\r\n\r\nThe core code is based on the MPI Task Farm model and the HDF5 data storage specification.\r\nIt may be installed system-wide and become a helper tool for users, who need to perform \r\na large number of serial computations. The code has been tested on large CPU-clusters, \r\nas well as desktop computers and works equally well (the Linux and Mac OS X operating \r\nsystems are actively maintained).\r\n\r\nMechanic is BSD-licensed. The source code package comes with few example\r\nmodules and is freely available at [the project page](http://git.ca.umk.pl).\r\n\r\n- [How does it work?](#how-does-it-work)\r\n- [Key features](#key-features)\r\n- [Notes](#notes)\r\n- [Example usage](#example-usage)\r\n- [Quick start](#quick-start)\r\n- [Differences to other task management systems](#differences-to-other-task-management-systems)\r\n- [Short history of Mechanic](#short-history-of-mechanic)\r\n- [Publications](#publications)\r\n- [Posters](#posters)\r\n- [Acknowledgments](#acknowledgments)\r\n\r\n\r\nHow does it work?\r\n-----------------\r\n\r\nConsider generation of an image (sets of points). In a single-threaded software,\r\nit requires a loop over all points (pixels):\r\n\r\n    for (i = 0; i < xdim; i++) {\r\n      for (j = 0; j < ydim; j++) {\r\n        result = Compute(i, j);\r\n      }\r\n    }\r\n\r\nIt works well, however, the problem arises, when the `Compute()` function takes a \r\nlong time to finish (especially in dynamical astronomy, the research field the Mechanic \r\ncame from). Since each `Compute()` call is independent, we may try to split the \r\nworkload by using some parallel techniques. In such a case, we will loop over all tasks \r\nto do, this time, however, each parallel thread will receive a single task, and return \r\nthe result to the master thread. This is what Mechanic does. It sends a single \r\n`Compute()` task to each worker node in the computing pool (say CPU cluster) and \r\ncombine the results (the so-called _MPI Task Farm_ model). Under the hood, the Mechanic \r\nis independent from the numerical problem itself -- it provides a unified way to create \r\nand access data files, setup, node-communication etc.\r\n\r\nKey features\r\n------------\r\n\r\n- **The numerical part of the code is fully separated from the setup and storage phase.** \r\n  You may use the user API to adjust the storage layout and configuration options\r\n- **Storage of data and configuration.** The core code takes care on the data storage \r\n  and configuration\r\n- **MPI Task Farm** model (master -- worker relationships between nodes in a computing\r\n  pool)\r\n- **The pool-based simulations.** Each pool may have different number of tasks to compute\r\n  (the task loop follows the _MPI Task Farm_ model), and data from all pools may be used \r\n  at the every stage of the simulation\r\n- **HDF5 data storage layout**\r\n- **All HDF5/MPI basic datatypes are supported**\r\n- **Multidimensional datasets support, with minimum rank 2 up to rank `H5S_MAX_RANK` (32)**\r\n- **Possibility of using HDF5 attributes (directly or through API)**\r\n- **Automatic backup of data files and restart mode**\r\n- **Configuration command line.** All configuration options defined through API are\r\n  automatically available in the command line\r\n- **Different storage modes** which allows to design the storage layout that fits best\r\n  the user needs (i.e. for processing with Gnuplot or Matplotlib)\r\n- **Linux and MAC OS X** supported\r\n\r\nNotes\r\n-----\r\n\r\n- The datasets are `H5S_SIMPLE`-type\r\n- The task board is rank 3 with the fourth dimension containing additional run information\r\n\r\nExample usage\r\n-------------\r\n\r\n- **Efficient creation of dynamical maps.** Each pixel of the map is mapped into\r\n  a standalone numerical task\r\n- **Genetic algorithms.** Each pool of tasks may be treated as a generation in the\r\n  language of GAs, and each task as a member of current population\r\n- **Data processing.** Think of processing of a huge number of astronomical observations\r\n\r\nQuick start\r\n-----------\r\n\r\nAs a quick and simple example consider creating a dynamical map (which is similar to image\r\nprocessing). A dynamical map helps to obtain global information of the dynamics of \r\nthe dynamical system. We would like to know the state of the system when we move from \r\nthe nominal solution (i.e. in the range of observations' uncertaintites). To do so, we \r\ncreate a grid of initial conditions (a map), which is close to the nominal solution. \r\nEach point of the map is a standalone numerical simulation, and we may speed up \r\nthe computations by splitting the workload within the farm model.  \r\n\r\nFirst, we would like to define some sensible storage layout for the result data. We will \r\nstore the location of the point of the map and the state of the system (1x3 array). Let us\r\ncreate a `mechanic_module_map.c` file and put in it the following code:\r\n\r\n    #include \"mechanic.h\"\r\n\r\n    int Storage(pool *p, setup *s) {\r\n      p->task->storage[0].layout = (schema) {\r\n        .name = \"result\", // the name of the output dataset\r\n        .rank = 2, // the rank of the dataset\r\n        .dim[0] = 1, // the vertical dimension of the result array \r\n        .dim[1] = 3, // the horizontal dimension of the result array \r\n        .use_hdf = 1, // whether to store the result in the master data file\r\n        .storage_type = STORAGE_PM3D, // storage type, which is suitable to process with Gnuplot PM3D\r\n        .datatype = H5T_NATIVE_DOUBLE, // the datatype\r\n      };\r\n\r\n      return SUCCESS;\r\n    }\r\n\r\nEach worker node will return such 1x3 array. The master node will receive those arrays \r\nand combine them into one result dataset.  We tell the Mechanic, that the output dataset \r\nshould be suitable to be processed with Gnuplot PM3D (STORAGE_PM3D flag). The size of \r\nthe output dataset in this case will be task_pool_size x 3.\r\n\r\nThe second step is to create the `TaskProcess()` function, in which we will compute the\r\nstate of the system:\r\n\r\n    int TaskProcess(pool *p, task *t, setup *s) {\r\n      double buffer[1][3];\r\n\r\n      buffer[0][0] = t->location[1]; // the vertical position of the current task\r\n      buffer[0][1] = t->location[0]; // the horizontal position of the current task\r\n      buffer[0][2] = t->tid; // task id represents the state of the system\r\n\r\n      // Write the buffer data to Mechanic's memory buffers\r\n      MWriteData(t, \"result\", &buffer[0][0]);\r\n\r\n      return SUCCESS;\r\n    }\r\n\r\nThe `data` array is automatically allocated using the information provided in the\r\n`Storage()` hook.\r\n\r\nOur state of the system is represented here by the unique task ID (you may use\r\nany numerical function here to get the result, of course). The task location is filled up\r\nautomatically during the task preparation phase. Each worker node receives a unique task\r\nto compute. \r\n\r\nWe should now compile our code to a shared library:\r\n\r\n    mpicc -fPIC -Dpic -shared mechanic_module_map.c -o libmechanic_module_map.so\r\n\r\nAfter all, we may run the code for a 10x10px map, using four MPI threads, one master \r\nand three workers:\r\n\r\n    mpirun -np 4 mechanic -p map -x 10 -y 10\r\n\r\nThe result is stored in the `mechanic-master-00.h5` file and may be accessed i.e. through\r\n`h5dump` utility. To list the contents of the file, try\r\n\r\n    # h5ls -r mechanic-master-00.h5\r\n\r\n    /                        Group\r\n    /LRC_Config              Type\r\n    /Pools                   Group\r\n    /Pools/last              Group\r\n    /Pools/last/Tasks        Group\r\n    /Pools/last/Tasks/result Dataset {100, 3}\r\n    /Pools/last/board        Dataset {10, 10}\r\n    /Pools/pool-0000         Group, same as /Pools/last\r\n    /config                  Group\r\n    /config/core             Group\r\n    /config/core/core        Dataset {13}\r\n    /config/module           Group\r\n    /config/module/core      Dataset {13}\r\n\r\nOur result is stored in the first task pool (only one task pool has been computed so far),\r\nin the dataset `/Pools/pool-0000/Tasks/result`:\r\n\r\n    # h5dump -d/Pools/pool-0000/Tasks/result mechanic-master-00.h5\r\n\r\n    DATASET \"/Pools/pool-0000/Tasks/result\" {\r\n       DATATYPE  H5T_IEEE_F64LE\r\n       DATASPACE  SIMPLE { ( 100, 3 ) / ( 100, 3 ) }\r\n       DATA {\r\n       (0,0): 0, 0, 0,\r\n       (1,0): 1, 0, 10,\r\n       (2,0): 2, 0, 20,\r\n       (3,0): 3, 0, 30,\r\n       (4,0): 4, 0, 40,\r\n       (5,0): 5, 0, 50,\r\n       (6,0): 6, 0, 60,\r\n       (7,0): 7, 0, 70,\r\n       (8,0): 8, 0, 80,\r\n       (9,0): 9, 0, 90,\r\n\r\n       (... the full output cutted off)\r\n    }\r\n\r\nDifferences to other task management systems\r\n--------------------------------------------\r\n\r\nThe Mechanic differs significantly from other task management systems, such as Condor \r\nor Workqueue, in terms of user API: our code does not use the executable of user's serial\r\ncode. Instead, the user's code should be rewritten within the provided API. In such a way, \r\nwe focus only on the numerical part of the task, and not its setup or storage. The HDF5\r\nstorage layer provides unified way to access the data by a number of different\r\napplications (not to mention C/Fortran codes only, but also Python software).\r\n\r\nShort history of Mechanic\r\n-------------------------\r\n\r\nThe idea of creating the Mechanic came from the problem of efficient computing of \r\ndynamical maps of planetary systems. Such maps consist of many many independent numerical \r\nsimulations which may take a long time (from seconds to weeks). Efficient splitting \r\nof such workload was neccessary to determine the global dynamics of exo-planets. \r\nThe MPI-software-skeleton-kind idea that came once to K. Goździewski's mind was expanded \r\nand developed by his PhD student, M. Słonina, as a part of his PHD thesis. The very first \r\nbranch of the Mechanic, the proof-of-concept 0.12.x, was successfully used on several\r\nclusters and became a good starting point for the full featured software.\r\n\r\nPublications\r\n------------\r\n\r\n- [Slonina M., Gozdziewski K., Migaszewski C., OCPD 125-129 (2012)](http://adsabs.harvard.edu/abs/2012ocpd.conf..125S)\r\n- [Migaszewski C., Slonina M., Gozdziewski K., 2012arXiv1205.0822M](http://arxiv.org/abs/1205.0822)\r\n- [Gozdziewski K. et al, MNRAS 425, 930–949 (2012)](http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2966.2012.21341.x/full)\r\n- [Gozdziewski K., Slonina M., Migaszewski C., Rozenkiewicz A., 2012arXiv1205.1341S](http://arxiv.org/abs/1205.1341)\r\n- [Gozdziewski K., Slonina M., Migaszewski C., OCPD 97-102 (2012)](http://adsabs.harvard.edu/abs/2012ocpd.conf...97G)\r\n- [Slonina M., Gozdziewski K., Migaszewski C., Astrophysics Source Code Library, record\r\nascl:1205.001](http://asterisk.apod.com/viewtopic.php?f=35&t=28482)\r\n\r\nPosters\r\n-------\r\n\r\n- Slonina M., Gozdziewski K., Migaszewski C., Simtech2011 (Stuttgart, June 2011)\r\n- Slonina M., Gozdziewski K., Migaszewski C., Orbital Couples: \"Pas de Deux\" in the Solar\r\n  System and the Milky Way (Paris, October 2011)\r\n\r\nAcknowledgments\r\n---------------\r\n\r\nThis project is supported by the Polish Ministry of Science and Higher Education through\r\nthe grant N/N203/402739. This work is conducted within the POWIEW project of the European\r\nRegional Development Fund in Innovative Economy Programme POIG.02.03.00-00-018/08.\r\n","tagline":"The MPI task management system for the dynamical astronomy","google":""}